# -*- coding: utf-8 -*-
"""Multi-Label.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h03zetwWB6EeP8Phg3tI1s2sl50AqtdF

Required packages:\
pandas==1.4.0\
numpy==1.21.5\
scikit-learn==1.0.2\
tensorflow==2.7.0\
torch==1.10.2\
transformers==4.17.0.dev0\
datasets==1.18.3\
textstat==0.7.2 (if running the ML part)\
xgboost==1.5.2 (if running the ML part)
"""

# pip install fsspec==2023.6.0
# pip install torch==2.2.1
# pip install accelerate==0.26.1
# pip install textstat
# pip install datasets
# python3 -m pip install pandas
# python3 -m pip install -U scikit-learn
# python3 -m pip install tensorflow
# python3 -m pip install transformers

import pandas as pd
import numpy as np

data = pd.read_csv("/home/ubuntu/esokli/CognitiveComplexityMoE/Data/sample_full.csv")

data.fillna({'Remember': 0, 'Understand': 0, 'Apply': 0, 'Analyze': 0, 'Evaluate': 0, 'Create':0}, inplace=True)

LIWC_data = pd.read_csv("/home/ubuntu/esokli/CognitiveComplexityMoE/Data/LIWC2015 Results (Learning_outcome.csv).csv")
data = data.join(LIWC_data).drop(['A'], axis=1)

data.head()

labels = data[data.columns[1:7]].values.tolist()

data.columns[1:7]

from sklearn.ensemble import RandomForestClassifier

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, cohen_kappa_score, f1_score

"""## BERT"""

import torch
import tensorflow as tf
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForSequenceClassification, EarlyStoppingCallback
from transformers import TFBertPreTrainedModel, TFBertMainLayer, InputFeatures
from datasets import load_metric, list_metrics
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EncodeDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', problem_type="multi_label_classification")
# model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6, problem_type="multi_label_classification")
model = AutoModelForSequenceClassification.from_pretrained('/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/checkpoint-520', num_labels=6, problem_type="multi_label_classification")

# Check if CUDA is available and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the model to the device
model = model.to(device)

train_x, test_x, train_y, test_y = train_test_split(data['Learning_outcome'].tolist(), labels, test_size=0.2, random_state=666)
train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=666)

train_encoded = tokenizer(train_x, truncation=True, padding=True, max_length=100)
val_encoded = tokenizer(val_x, truncation=True, padding=True, max_length=100)
test_encoded = tokenizer(test_x, truncation=True, padding=True, max_length=100)

train_set, val_set, test_set = EncodeDataset(train_encoded, train_y), EncodeDataset(val_encoded, val_y), EncodeDataset(test_encoded, test_y)

training_args = TrainingArguments(
        output_dir='/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier',          # output directory
        overwrite_output_dir=True,
        num_train_epochs=3,              # total number of training epochs
        per_device_train_batch_size=64,  # batch size per device during training
        per_device_eval_batch_size=64,   # batch size for evaluation
        warmup_steps=5,                # number of warmup steps for learning rate scheduler
        weight_decay=0.05,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=10,
        evaluation_strategy="steps",
        save_strategy="steps",
        save_steps=10,
        load_best_model_at_end=True
    )

def getClassResult(predicted):
    results = []
    for probs in predicted.numpy():
        result = []
        for prob in probs:
            if prob < 0.5:
                result.append(0)
            else:
                result.append(1)
        results.append(result)
    return results

metric = load_metric("f1")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = tf.keras.activations.sigmoid(logits)
    predicted = getClassResult(predictions)
    return metric.compute(predictions=predicted, references=labels, average="micro")

trainer = Trainer(model=model, args=training_args, train_dataset=train_set, eval_dataset=val_set, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])

trainer.train()

logits = trainer.predict(test_set)

logits.predictions.shape

predicted = tf.keras.activations.sigmoid(logits.predictions)

predicted.numpy()

predicted_label = getClassResult(predicted)

count = 0
for pred in predicted_label:
    if pred.count(1) > 1:
        count += 1
count

print(classification_report(test_y, predicted_label, output_dict=False, target_names=list(data.columns[1:7]), digits=3))

roc_auc_score(test_y, predicted.numpy(), average=None)

accuracy_score(np.array(test_y), predicted_label)

dl_result_df = pd.DataFrame(data=predicted_label, columns=data.columns[1:7])

# print(accuracy_score(ml_golden_df['Remember'].tolist(), dl_result_df['Remember'].tolist()))
# print(accuracy_score(ml_golden_df['Understand'].tolist(), dl_result_df['Understand'].tolist()))
# print(accuracy_score(ml_golden_df['Apply'].tolist(), dl_result_df['Apply'].tolist()))
# print(accuracy_score(ml_golden_df['Analyze'].tolist(), dl_result_df['Analyze'].tolist()))
# print(accuracy_score(ml_golden_df['Evaluate'].tolist(), dl_result_df['Evaluate'].tolist()))
# print(accuracy_score(ml_golden_df['Create'].tolist(), dl_result_df['Create'].tolist()))

# print(cohen_kappa_score(ml_golden_df['Remember'].tolist(), dl_result_df['Remember'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Understand'].tolist(), dl_result_df['Understand'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Apply'].tolist(), dl_result_df['Apply'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Analyze'].tolist(), dl_result_df['Analyze'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Evaluate'].tolist(), dl_result_df['Evaluate'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Create'].tolist(), dl_result_df['Create'].tolist()))

# Load the datasets
logger.info("Loading datasets...")
queries = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/queries.csv')
passages = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/passages.csv')

# Filter the 'passages' dataframe
logger.info("Filtering passages...")
passages_filtered = passages[passages['passage_param'] == 'passage_text']

# Function to predict labels using BERT
def predict_labels(texts, batch_size=16):
    logger.info("Predicting labels...")
    preds = []
    for i in range(0, len(texts), batch_size):
        logger.info(f"Processing batch {i//batch_size + 1}/{len(texts)//batch_size + 1}...")
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}  # move inputs to device
        with torch.no_grad():
            outputs = model(**inputs)
        _, batch_preds = torch.max(outputs.logits, dim=1)
        preds.extend(batch_preds.tolist())
    return preds

# Predict labels for the 'queries' and 'passages' datasets
logger.info("Predicting labels for queries...")
queries['label'] = predict_labels(queries['query'])
logger.info("Saving queries results...")
queries.to_csv('queries_with_labels.csv', index=False)

logger.info("Predicting labels for passages...")
passages_filtered['label'] = predict_labels(passages_filtered['passage_val'])

# Merge the predicted labels back into the original 'passages' DataFrame
logger.info("Merging predicted labels for passages...")
passages = passages.merge(passages_filtered[['label']], left_index=True, right_index=True, how='left')

logger.info("Saving passages results...")
passages.to_csv('passages_with_labels.csv', index=False)

logger.info("Done.")

# """## ML Test"""
# import textstat

# def generateX(data_x, test_x, textual_column_index, start_index_LIWC, end_index_LIWC):
#     column_names = []
#     print("Getting Unigram...")
#     uni_cv = CountVectorizer(stop_words='english', ngram_range=(1, 1), max_features=1000)
#     unigram = uni_cv.fit_transform(data_x[:, textual_column_index])
#     unigram = unigram.toarray()
#     unigram_test = uni_cv.transform(test_x[:,textual_column_index]).toarray()
#     temp = uni_cv.get_feature_names_out().tolist()
#     column_names += ["uni_"+name for name in temp]
#     print("Getting Bigram...")
#     bi_cv = CountVectorizer(stop_words='english', ngram_range=(2, 2), max_features=1000)
#     bigram = bi_cv.fit_transform(data_x[:, textual_column_index])
#     bigram = bigram.toarray()
#     bigram_test = bi_cv.transform(test_x[:, textual_column_index]).toarray()
#     temp = bi_cv.get_feature_names_out().tolist()
#     column_names += ["bi_"+name for name in temp]
#     print("Getting Tfidf...")
#     tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=1000)
#     t = tfidf.fit_transform(data_x[:, textual_column_index])
#     t = t.toarray()
#     t_test = tfidf.transform(test_x[:, textual_column_index]).toarray()
#     temp = tfidf.get_feature_names_out().tolist()
#     column_names += ["tfidf_"+name for name in temp]
#     print("Getting ARI...")
#     ari = [textstat.automated_readability_index(text) for text in data_x[:, textual_column_index]]
#     ari_test = [textstat.automated_readability_index(text) for text in test_x[:, textual_column_index]]
#     column_names.append("ari")
#     combined_data_x = []
#     combined_test_x = []
#     print("Combining...")
#     for i in range(len(data_x)):
#         combined_data_x.append(unigram[i].tolist()
#                               + bigram[i].tolist()
#                               + t[i].tolist()
#                               + [ari[i]]
#                               + data_x[i, start_index_LIWC:end_index_LIWC].tolist())
#     for i in range(len(test_x)):
#         combined_test_x.append(unigram_test[i].tolist()
#                               + bigram_test[i].tolist()
#                               + t_test[i].tolist()
#                               + [ari_test[i]]
#                               + test_x[i, start_index_LIWC:end_index_LIWC].tolist())
#     print("Generated feature shape is", np.array(combined_data_x).shape)
#     print("Generated test feature is", np.array(combined_test_x).shape)
#     return combined_data_x, column_names, combined_test_x

# data.drop(columns=list(data.columns[1:7])).iloc[:, 0]

# train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=list(data.columns[1:8])), data[data.columns[1:7]], test_size=0.2, random_state=666)

# np.unique(train_y['Remember'].tolist(), return_counts=True), np.unique(test_y['Remember'].tolist(), return_counts=True)

# np.unique(train_y['Understand'].tolist(), return_counts=True), np.unique(test_y['Understand'].tolist(), return_counts=True)

# np.unique(train_y['Apply'].tolist(), return_counts=True), np.unique(test_y['Apply'].tolist(), return_counts=True)

# np.unique(train_y['Analyze'].tolist(), return_counts=True), np.unique(test_y['Analyze'].tolist(), return_counts=True)

# np.unique(train_y['Evaluate'].tolist(), return_counts=True), np.unique(test_y['Evaluate'].tolist(), return_counts=True)

# np.unique(train_y['Create'].tolist(), return_counts=True), np.unique(test_y['Create'].tolist(), return_counts=True)

# one_hot = []
# for d in data[data.columns[1:7]].values:
#     one_hot.append(np.array2string(d).count("1"))
# np.unique(one_hot, return_counts=True)

# ml_train_x, column_names, ml_test_x = generateX(train_x.to_numpy(), test_x.to_numpy(), 0, 1, 94)

# column_names += data.columns[7:].tolist()

# rf = RandomForestClassifier()
# rf.fit(ml_train_x, train_y)

# pred_y = rf.predict(ml_test_x)

# print(classification_report(test_y, pred_y, output_dict=False, target_names=list(data.columns[1:7]), digits=3))

# pred_score_y = rf.predict_proba(ml_test_x)

# np.array(test_x).shape

# np.array(pred_score_y).shape

# pred_score_y = np.transpose([score[:, 1] for score in rf.predict_proba(ml_test_x)])

# roc_auc_score(test_y, pred_score_y, average=None)

# f1_score(test_y, pred_y, average="micro")

# accuracy_score(test_y, pred_y)

# ml_result_df = pd.DataFrame(data=pred_y, columns=data.columns[1:7])

# ml_result_df

# ml_golden_df = pd.DataFrame(data=test_y, columns=data.columns[1:7])

# print(accuracy_score(ml_golden_df['Remember'].tolist(), ml_result_df['Remember'].tolist()))
# print(accuracy_score(ml_golden_df['Understand'].tolist(), ml_result_df['Understand'].tolist()))
# print(accuracy_score(ml_golden_df['Apply'].tolist(), ml_result_df['Apply'].tolist()))
# print(accuracy_score(ml_golden_df['Analyze'].tolist(), ml_result_df['Analyze'].tolist()))
# print(accuracy_score(ml_golden_df['Evaluate'].tolist(), ml_result_df['Evaluate'].tolist()))
# print(accuracy_score(ml_golden_df['Create'].tolist(), ml_result_df['Create'].tolist()))

# print(cohen_kappa_score(ml_golden_df['Remember'].tolist(), ml_result_df['Remember'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Understand'].tolist(), ml_result_df['Understand'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Apply'].tolist(), ml_result_df['Apply'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Analyze'].tolist(), ml_result_df['Analyze'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Evaluate'].tolist(), ml_result_df['Evaluate'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Create'].tolist(), ml_result_df['Create'].tolist()))
